{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpGv7oW4PwdM"
   },
   "source": [
    "# Exercise 2: Neural Networks\n",
    "\n",
    "In the previous exercise you implemented a classifier with one linear layer. In this exercise, you will implement a three layer multi-class neural network.\n",
    "\n",
    "## Submission guidelines:\n",
    "\n",
    "Your submission should only include this jupyter notebook named ex2_ID.ipynb (not in zip).\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write **efficient vectorized** code whenever possible. \n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Do not change the functions we provided you. \n",
    "4. Write your functions in the instructed python modules only. All the logic you write is imported and used using this jupyter notebook. You are allowed to add functions as long as they are located in the python modules and are imported properly.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden.\n",
    "6. Your code must run without errors.\n",
    "7. **Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works. You should include your desired outputs in the output cells to make your code easier to understand.**\n",
    "8. Write your own code. Cheating will not be tolerated. \n",
    "9. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support).\n",
    "\n",
    "**TIP:** You may find the following link helpful: \n",
    "http://cs231n.github.io/neural-networks-case-study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.152048Z",
     "start_time": "2022-11-28T06:46:17.469849Z"
    },
    "id": "pA0hjtyJPwdO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "# specify the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcYX2rU2PwdP"
   },
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa-bjEWl9fW6"
   },
   "source": [
    "## Data download and processing Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.167551Z",
     "start_time": "2022-11-28T06:46:18.153952Z"
    },
    "id": "9_uAlYcQ9dF7"
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        print(\"Downloading, This might take several minutes.\")\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "        print(\"If not, delete the dataset folder and try again.\")\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding = 'latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MeVWzF19mVA"
   },
   "source": [
    "## Data Download\n",
    "\n",
    "The next cell will download and extract CIFAR-10 into `datasets/cifar10/`. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly-selected images from each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:18.183294Z",
     "start_time": "2022-11-28T06:46:18.170425Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bHbknwvPwdQ",
    "outputId": "f5fb53a7-aca0-4360-9196-453dfbb1904d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "If not, delete the dataset folder and try again.\n"
     ]
    }
   ],
   "source": [
    "# this cell will download the data if it does not exists\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "PATH = 'datasets/cifar10/' # the script will create required directories\n",
    "maybe_download_and_extract(URL, PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beFt0AfB9w9f"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YTIdyB5qMO"
   },
   "source": [
    "**Notice that we are leaving behind the bias trick in this exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:20.967912Z",
     "start_time": "2022-11-28T06:46:18.185329Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyLpzTyzPwdQ",
    "outputId": "3508364b-41bd-43ab-d7df-ea78a23a23e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (20000, 3072)\n",
      "Shape of validation set: (1000, 3072)\n",
      "Shape of test set: (1000, 3072)\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
    "X_train, y_train, X_test, y_test = load(CIFAR10_PATH) # load the entire data\n",
    "num_classes = 4\n",
    "\n",
    "X_train = X_train[np.isin(y_train, range(num_classes))]\n",
    "y_train = y_train[np.isin(y_train, range(num_classes))]\n",
    "X_test = X_test[np.isin(y_test, range(num_classes))]\n",
    "y_test = y_test[np.isin(y_test, range(num_classes))]\n",
    "\n",
    "# define a splitting for the data\n",
    "num_training = num_classes*5000\n",
    "num_validation = 1000\n",
    "num_testing = 1000\n",
    "\n",
    "# add a validation dataset for hyperparameter optimization\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "mask = range(num_validation)\n",
    "X_val = X_test[mask]\n",
    "y_val = y_test[mask]\n",
    "mask = range(num_validation, num_validation+num_testing)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# float64\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_val = X_val.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# subtract the mean from all the images in the batch\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# flatten all the images in the batch (make sure you understand why this is needed)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, newshape=(X_val.shape[0], -1)) \n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], -1)) \n",
    "\n",
    "print(f\"Shape of training set: {X_train.shape}\")\n",
    "print(f\"Shape of validation set: {X_val.shape}\")\n",
    "print(f\"Shape of test set: {X_test.shape}\")\n",
    "classes = ['plane', 'car', 'bird', 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.137947Z",
     "start_time": "2022-11-28T06:46:20.972016Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "mHATPoNJPwdQ",
    "outputId": "5a2e0694-1dbc-43ef-892a-e811842c11ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bird     plane     plane     plane\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1200 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_batch(X, y, n=1000):\n",
    "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
    "    images = X[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    return images, labels\n",
    "\n",
    "def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n",
    "    if random_flag:\n",
    "        rand_items = np.random.randint(0, x.shape[0], size=n)\n",
    "    else:\n",
    "        rand_items = np.arange(0, x.shape[0])\n",
    "    images = x[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    if convert_to_image:\n",
    "        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n",
    "    else:\n",
    "        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n",
    "    print(' '.join('%9s' % classes[labels[j]] for j in range(n)))\n",
    "    return grid\n",
    "\n",
    "def vec_2_img(x):\n",
    "    x = np.reshape(x, (32, 32, 3))\n",
    "    return x\n",
    "\n",
    "X_batch, y_batch = get_batch(X_test, y_test, 100)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch, convert_to_image=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9MRQD6v4aDz"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRuXmTuzPwdS"
   },
   "source": [
    "## Cross-entropy\n",
    "\n",
    "\n",
    "Complete the function `softmax_loss` using vectorized code. This function takes as input `scores`, labels `y` and outputs the calculated loss as a single number and the gradients with respect to X. **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.153755Z",
     "start_time": "2022-11-28T06:46:21.140549Z"
    },
    "id": "GQ2BtZDMOaMv"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(scores, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - scores: scores of shape (N, C) where scores[i, c] is the score for class c on input X[i].\n",
    "    - y: Vector of labels\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement this function                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    num_examples = scores.shape[0]\n",
    "    \n",
    "    probs = get_probs(scores)\n",
    "    # -log probs of correct class\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    \n",
    "    # compute the loss: average loss\n",
    "    data_loss = np.sum(correct_logprobs) / num_examples\n",
    "    loss = data_loss\n",
    "    \n",
    "    # gradient on the scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "    dx = dscores\n",
    "\n",
    "    #print(loss)\n",
    "    #print(dx)\n",
    "\n",
    "   \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                              END OF YOUR CODE                           #\n",
    "    ###########################################################################\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to get probs from scores\n",
    "def get_probs(scores):\n",
    "    num_examples = scores.shape[0]\n",
    "    # get unnormalized probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    # normalize them for each example\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    return probs"
   ],
   "metadata": {
    "id": "Kx9tK8kFJlwT"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.169038Z",
     "start_time": "2022-11-28T06:46:21.155754Z"
    },
    "id": "VadKXfqI8o1i"
   },
   "outputs": [],
   "source": [
    "# some tests\n",
    "np.random.seed(42)\n",
    "\n",
    "num_instances = 5\n",
    "num_classes = 3\n",
    "\n",
    "y = np.random.randint(num_classes, size=num_instances)\n",
    "scores = np.random.randn(num_instances * num_classes).reshape(num_instances, num_classes)\n",
    "loss, dx = softmax_loss(scores, y)\n",
    "\n",
    "\n",
    "correct_grad = np.array([[ 0.0062,  0.1751, -0.1813],\n",
    "         [-0.1463,  0.0561,  0.0901],\n",
    "         [ 0.0404,  0.0771, -0.1174],\n",
    "         [ 0.0223,  0.0855, -0.1078],\n",
    "         [-0.1935,  0.1358,  0.0578]])\n",
    "correct_loss = 1.7544\n",
    "assert np.isclose(dx.round(4), correct_grad, rtol=1e-3).all()\n",
    "assert np.isclose(loss.round(4), correct_loss, rtol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3b1r34XiywH"
   },
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term to the loss to penalize larger weights. \n",
    "$$\n",
    "Loss = Loss + \\lambda  \\cdot \\frac{1}{2} \\cdot \\sum_{i=0}^k w_k^2\n",
    "$$\n",
    "\n",
    "Implement the L2 regularization part of the loss in the next cell: **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.184401Z",
     "start_time": "2022-11-28T06:46:21.171489Z"
    },
    "id": "wk--K9pGi_Lb"
   },
   "outputs": [],
   "source": [
    "def l2_regulariztion_loss(W, reg=0):\n",
    "    \"\"\"\n",
    "    L2 regulariztion loss function, vectorized version.\n",
    "    - W: a layer's weights.\n",
    "    - reg: (float) regularization strength\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    #############################################################################\n",
    "    # TODO: Compute the L2 reulariztion loss and its gradient using no \n",
    "    # explicit loops.                                                           #\n",
    "    # Store the loss in loss and the gradient in dW.                            #\n",
    "    #############################################################################\n",
    "    reg_loss = 0.5 * reg * np.sum(W*W)\n",
    "    \n",
    "    dW += reg*W # don't forget the regularization gradient\n",
    "    loss = reg_loss\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xerXQe6hPwdU"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "The implementation of linear regression was (hopefully) simple yet not very modular since the layer, loss and gradient were calculated as a single monolithic function. This would become impractical as we move towards bigger models. As a warmup towards `PyTorch`, we want to build networks using a more modular design so that we can implement different layer types in isolation and easily integrate them together into models with different architectures.\n",
    "\n",
    "This logic of isolation & integration is at the heart of all popular deep learning frameworks, and is based on two methods each layer holds - a forward and backward pass. The forward function will receive inputs, weights and other parameters and will return both an output and a cache object storing data needed for the backward pass. The backward pass will receive upstream derivatives and the cache, and will return gradients with respect to the inputs and weights. By implementing several types of layers this way, we will be able to easily combine them to build classifiers with different architectures with relative ease.\n",
    "\n",
    "We will implement a neural network to obtain better results on CIFAR-10. \n",
    "Our neural network will be implemented in the following cells. We will train this network using softmax loss and L2 regularization and a ReLU non-linearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHH9dGgt4NP3"
   },
   "source": [
    "### Fully Connected Layer: Forward Pass. \n",
    "\n",
    "Implement the function `fc_forward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.200656Z",
     "start_time": "2022-11-28T06:46:21.189432Z"
    },
    "id": "OO3vgLtGVWkv"
   },
   "outputs": [],
   "source": [
    "def fc_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an fully connected layer.\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - W: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    x_reshaped = reshape_features_dim(X)\n",
    "    out = np.dot(x_reshaped, W) + b\n",
    "    #cache = (X.copy(), W.copy(), b.copy())\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (X.copy(), W.copy(), b.copy())\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Flatten features\n",
    "def reshape_features_dim(X):\n",
    "    num_examples = X.shape[0]\n",
    "    num_features =  get_num_features(X)\n",
    "    x_reshaped = X.reshape(num_examples, num_features)\n",
    "    return x_reshaped"
   ],
   "metadata": {
    "id": "BOlVic91WeMB"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_num_features(X):\n",
    "    num_examples = X.shape[0]\n",
    "    d_arr = X.shape[1:]\n",
    "    num_features = np.prod(d_arr)\n",
    "    return num_features"
   ],
   "metadata": {
    "id": "0_Pjllo92O66"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.215718Z",
     "start_time": "2022-11-28T06:46:21.202326Z"
    },
    "id": "bkwG7GJ_PwdU"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_instances = 5\n",
    "input_shape = (11, 7, 3)\n",
    "output_shape = 4\n",
    "\n",
    "X = np.random.randn(num_instances * np.prod(input_shape)).reshape(num_instances, *input_shape)\n",
    "W = np.random.randn(np.prod(input_shape) * output_shape).reshape(np.prod(input_shape), output_shape)\n",
    "b = np.random.randn(output_shape)\n",
    "\n",
    "out, _ = fc_forward(X, W, b)\n",
    "\n",
    "\n",
    "\n",
    "correct_out = np.array([[16.77132953,  1.43667172, -15.60205534,   7.15789287],\n",
    "                        [ -8.5994206,  7.59104298,  10.92160126,  17.19394331],\n",
    "                        [ 4.77874003,  2.25606192,  -6.10944859,  14.76954561],\n",
    "                        [21.21222953, 17.82329258,   4.53431782,  -9.88327913],\n",
    "                        [18.83041801, -2.55273817,  14.08484003,  -3.99196171]])\n",
    "\n",
    "assert np.isclose(out, correct_out, rtol=1e-8).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaJgjBa2PwdU"
   },
   "source": [
    "### Fully Connected Layer: Backward Pass \n",
    "\n",
    "Implement the function `fc_backward` **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.231622Z",
     "start_time": "2022-11-28T06:46:21.217581Z"
    },
    "id": "GKGLxK7wVakI"
   },
   "outputs": [],
   "source": [
    "def fc_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an fully connected layer.\n",
    "    Try the link in the exercise intructions for more details.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: Tuple of:\n",
    "      - X: Input data\n",
    "      - W: Weights\n",
    "      - b: Biases\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to X\n",
    "    - dw: Gradient with respect to W\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = 0, 0, 0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    x_reshaped = reshape_features_dim(x)\n",
    "    # backpropate the gradient \n",
    "    dW = np.dot(x_reshaped.T, dout)\n",
    "    db = np.sum(dout, axis=0, keepdims=False)\n",
    "    dx = dout.dot(w.T).reshape(x.shape)\n",
    "\n",
    "    #print(\"dx = {}\".format(dx))\n",
    "    #print(\"dW = {}\".format(dW))\n",
    "    #print(\"db = {}\".format(db))\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.247332Z",
     "start_time": "2022-11-28T06:46:21.232614Z"
    },
    "id": "K3Xcoeqxnq_F"
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.262753Z",
     "start_time": "2022-11-28T06:46:21.248711Z"
    },
    "id": "7J13imMzPwdU"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = fc_forward(x,w,b)\n",
    "dx, dw, db = fc_backward(dout, cache)\n",
    "\n",
    "\n",
    "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6D5b-J5PwdV"
   },
   "source": [
    "### ReLU: Forward Pass \n",
    "\n",
    "Implement the function `relu_forward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.278815Z",
     "start_time": "2022-11-28T06:46:21.264744Z"
    },
    "id": "bO5iMs3aVeTl"
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "\n",
    "    out = np.maximum(0, x)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = x.copy()\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.294685Z",
     "start_time": "2022-11-28T06:46:21.280962Z"
    },
    "id": "WdYx8zaOPwdV"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "assert np.isclose(out, correct_out, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOP-MRufPwdV"
   },
   "source": [
    "### ReLU: Backward Pass\n",
    "\n",
    "Implement the function `relu_backward`. **(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.310754Z",
     "start_time": "2022-11-28T06:46:21.296684Z"
    },
    "id": "n3YsNEphVhuo"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "\n",
    "    local_derivative = np.zeros_like(x)\n",
    "    local_derivative[x > 0] = 1\n",
    "    dx = local_derivative * dout\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.325592Z",
     "start_time": "2022-11-28T06:46:21.312932Z"
    },
    "id": "w_9OFZZ-PwdV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "xx, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all()  # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX0UZJdDPwdV"
   },
   "source": [
    "### Combined Layer\n",
    "Next combine the fully connected and relu forward\\backward functions togther using the functions in the following cell. \n",
    "Remember to use functions you already implemented.\n",
    "**(5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.341148Z",
     "start_time": "2022-11-28T06:46:21.327148Z"
    },
    "id": "yhSV6tHgpZd0"
   },
   "outputs": [],
   "source": [
    "def fc_relu_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a fully connected layer followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input to the fc layer\n",
    "    - W, b: Weights for the fc layer\n",
    "\n",
    "    Returns:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO: Implement the function.                                             #\n",
    "    #############################################################################\n",
    "\n",
    "    fc_out, fc_cache = fc_forward(X, W, b)\n",
    "    relu_out, relu_cache = relu_forward(fc_out)\n",
    "    out = relu_out\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a fully connected layer followed by a ReLU\n",
    "    Inputs:\n",
    "    - dout: upstream derivatives\n",
    "    - cache: parameters calculated during the forward pass\n",
    "\n",
    "    Returns:\n",
    "    - dX: derivative w.r.t X\n",
    "    - dW: derivative w.r.t W\n",
    "    - db: derivative w.r.t b\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the function.                                             #\n",
    "    #############################################################################\n",
    "\n",
    "    relu_dout = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = fc_backward(relu_dout, fc_cache)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RNFsQVGrFUE"
   },
   "source": [
    "You can check your results in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.357151Z",
     "start_time": "2022-11-28T06:46:21.343147Z"
    },
    "id": "wfsIy8dEqx7r"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: fc_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: fc_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: fc_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "out, cache = fc_relu_forward(x,w,b)\n",
    "dx, dw, db = fc_relu_backward(dout, cache)\n",
    "\n",
    "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
    "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHYeDvNcPwdV"
   },
   "source": [
    "# Building the Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7-k0EkePwdV"
   },
   "source": [
    "Complete the class `ThreeLayerNet`. **(35 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.388140Z",
     "start_time": "2022-11-28T06:46:21.358811Z"
    },
    "id": "883fce5uWDVl"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer fully-connected neural network. This network has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "    In our case, we use the same hidden dimension across all hidden layers.\n",
    "    We train the network with a softmax loss function and L2 regularization on the\n",
    "    weight matrices. In other words, the network has the following architecture:\n",
    "\n",
    "    input - fc layer - ReLU - fc layer - ReLu - fc layer - softmax\n",
    "\n",
    "    The outputs of the third fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-2):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, H)\n",
    "        b2: Second layer biases; has shape (H,)\n",
    "        W3: Second layer weights; has shape (H, C)\n",
    "        b3: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in each of the hidden layers.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    def step(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a three layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization coefficient.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3'] \n",
    "\n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "        # Store the result in the scores variable, which should be an array of      #\n",
    "        # shape (N, C).                                                             #\n",
    "        #############################################################################\n",
    "        scores_1, cache_1 = fc_relu_forward(X, W1, b1)\n",
    "        scores_2, cache_2 = fc_relu_forward(scores_1, W2, b2)\n",
    "        #scores_3, cache_3 = fc_relu_forward(scores_2, W3, b3)\n",
    "        scores_3, cache_3 = fc_forward(scores_2, W3, b3)\n",
    "        \n",
    "        scores = scores_3\n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "        ###############################################################################\n",
    "        # After you finished the forward pass, compute the loss. This should include  #\n",
    "        # both the data loss and L2 regularization for W1, W2, W3. Store the result   #\n",
    "        # in the variable loss, which should be a scalar. Use the softmax_loss        #\n",
    "        # and l2_regulariztion_loss functions you implemented.                        #         \n",
    "        ###############################################################################\n",
    "        soft_loss, soft_dx = softmax_loss(scores, y)\n",
    "\n",
    "        reg_loss1, reg_dw1 = l2_regulariztion_loss(W1, reg)\n",
    "        reg_loss2, reg_dw2 = l2_regulariztion_loss(W2, reg)\n",
    "        reg_loss3, reg_dw3 = l2_regulariztion_loss(W3, reg)\n",
    "        reg_loss = reg_loss1 + reg_loss2 + reg_loss3\n",
    "        \n",
    "        loss = soft_loss + reg_loss\n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "        #############################################################################\n",
    "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "        # and biases. Store the results in the grads dictionary. For example,       #\n",
    "        # grads['W1'] = dW1 + dW1_reg, it stores the gradient on W1, including      #\n",
    "        # regularization. It should be a matrix of the same size.                   #\n",
    "        #############################################################################\n",
    "\n",
    "        dx3, dW3, db3 = fc_backward(soft_dx, cache_3)\n",
    "        #dx3, dW3, db3 = fc_relu_backward(soft_dx, cache_3)\n",
    "        dx2, dW2, db2 = fc_relu_backward(dx3, cache_2)\n",
    "        dx1, dW1, db1 = fc_relu_backward(dx2, cache_1)\n",
    "\n",
    "        grads['W1'] = dW1 + reg_dw1\n",
    "        grads['b1'] = db1\n",
    "        grads['W2'] = dW2 + reg_dw2\n",
    "        grads['b2'] = db2\n",
    "        grads['W3'] = dW3 + reg_dw3\n",
    "        grads['b3'] = db3\n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training label.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            #########################################################################\n",
    "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "            # them in X_batch and y_batch respectively.                             #\n",
    "            #########################################################################\n",
    "            \n",
    "            rand_items = np.random.randint(0, num_train, size=batch_size)\n",
    "            X_batch = X[rand_items]\n",
    "            y_batch = y[rand_items]\n",
    "            \n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.step(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "            # parameters of the network (stored in the dictionary self.params)      #\n",
    "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "            # stored in the grads dictionary defined above.                         #\n",
    "            #########################################################################\n",
    "\n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['b1'] -= (learning_rate * grads['b1'])#.reshape(self.params['b1'].shape)\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b2'] -= (learning_rate * grads['b2'])#.reshape(self.params['b2'].shape)\n",
    "            self.params['W3'] -= learning_rate * grads['W3']\n",
    "            self.params['b3'] -= (learning_rate * grads['b3'])#.reshape(self.params['b3'].shape)\n",
    "\n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and (it+1) % 100 == 0:\n",
    "                print ('iteration %d / %d: loss %f' % (it+1, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this three-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: data points to classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: predicted labels\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3'] \n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Implement this function                                           #\n",
    "        ###########################################################################\n",
    "        \n",
    "        scores_1, cache_1 = fc_relu_forward(X, W1, b1)\n",
    "        scores_2, cache_2 = fc_relu_forward(scores_1, W2, b2)\n",
    "        scores_3, cache_3 = fc_relu_forward(scores_2, W3, b3)\n",
    "        \n",
    "        probs = get_probs(scores_3)\n",
    "        y_pred = np.argmax(probs, axis = 1)\n",
    "        \n",
    "        ###########################################################################\n",
    "        #                              END OF YOUR CODE                           #\n",
    "        ###########################################################################\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:46:21.403237Z",
     "start_time": "2022-11-28T06:46:21.390103Z"
    },
    "id": "i6KXQ1KvPwdV"
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 128\n",
    "num_classes = 4\n",
    "model = ThreeLayerNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:07.708024Z",
     "start_time": "2022-11-28T06:46:21.405440Z"
    },
    "id": "XbUwlaa9PwdV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e26c5a07-53bf-48d7-962c-f412542a736b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 1.084206\n",
      "iteration 200 / 1500: loss 1.004476\n",
      "iteration 300 / 1500: loss 0.900380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m stats \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_iters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[23], line 180\u001B[0m, in \u001B[0;36mThreeLayerNet.train\u001B[1;34m(self, X, y, X_val, y_val, learning_rate, reg, num_iters, batch_size, verbose)\u001B[0m\n\u001B[0;32m    173\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m y[rand_items]\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;66;03m#                             END OF YOUR CODE                          #\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \n\u001B[0;32m    179\u001B[0m \u001B[38;5;66;03m# Compute loss and gradients using the current minibatch\u001B[39;00m\n\u001B[1;32m--> 180\u001B[0m loss, grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    181\u001B[0m loss_history\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m    183\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# TODO: Use the gradients in the grads dictionary to update the         #\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;66;03m# parameters of the network (stored in the dictionary self.params)      #\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;66;03m# using stochastic gradient descent. You'll need to use the gradients   #\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;66;03m# stored in the grads dictionary defined above.                         #\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m#########################################################################\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[23], line 99\u001B[0m, in \u001B[0;36mThreeLayerNet.step\u001B[1;34m(self, X, y, reg)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m###############################################################################\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# After you finished the forward pass, compute the loss. This should include  #\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# both the data loss and L2 regularization for W1, W2, W3. Store the result   #\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# in the variable loss, which should be a scalar. Use the softmax_loss        #\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# and l2_regulariztion_loss functions you implemented.                        #         \u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m###############################################################################\u001B[39;00m\n\u001B[0;32m     97\u001B[0m soft_loss, soft_dx \u001B[38;5;241m=\u001B[39m softmax_loss(scores, y)\n\u001B[1;32m---> 99\u001B[0m reg_loss1, reg_dw1 \u001B[38;5;241m=\u001B[39m \u001B[43ml2_regulariztion_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m reg_loss2, reg_dw2 \u001B[38;5;241m=\u001B[39m l2_regulariztion_loss(W2, reg)\n\u001B[0;32m    101\u001B[0m reg_loss3, reg_dw3 \u001B[38;5;241m=\u001B[39m l2_regulariztion_loss(W3, reg)\n",
      "Cell \u001B[1;32mIn[9], line 14\u001B[0m, in \u001B[0;36ml2_regulariztion_loss\u001B[1;34m(W, reg)\u001B[0m\n\u001B[0;32m      8\u001B[0m dW \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros_like(W)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#############################################################################\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# TODO: Compute the L2 reulariztion loss and its gradient using no \u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# explicit loops.                                                           #\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Store the loss in loss and the gradient in dW.                            #\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#############################################################################\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m reg_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m reg \u001B[38;5;241m*\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m dW \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reg\u001B[38;5;241m*\u001B[39mW \u001B[38;5;66;03m# don't forget the regularization gradient\u001B[39;00m\n\u001B[0;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m reg_loss\n",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36msum\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stats = model.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1500, batch_size=200,\n",
    "            learning_rate=1e-3, reg=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:07.754799Z",
     "start_time": "2022-11-28T06:47:07.711049Z"
    },
    "id": "-1hDhsb1PwdV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46b8c675-af92-4aac-e626-7c71d21235a2"
   },
   "outputs": [],
   "source": [
    "val_acc = (model.predict(X_val) == y_val).mean()\n",
    "print ('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:08.208742Z",
     "start_time": "2022-11-28T06:47:07.756434Z"
    },
    "id": "VdhExUOrKcc6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0964c121-2731-41f9-c0ae-a980afb7d803"
   },
   "outputs": [],
   "source": [
    "train_acc = (model.predict(X_train) == y_train).mean()\n",
    "print ('Training accuracy: ', train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T06:47:08.619195Z",
     "start_time": "2022-11-28T06:47:08.210823Z"
    },
    "id": "F-9rpmQAPwdW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "outputId": "e63d7bfd-f260-4ab5-8157-d74d16c74863"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Okkhr5xjPwdW"
   },
   "source": [
    "## Hyperparameter Optimization\n",
    "Use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, hidden_size, regularization)` to tuples of the form `(training_accuracy, validation_accuracy)`. You should evaluate the best model on the testing dataset and print out the training, validation and testing accuracies for each of the models and provide a clear visualization. Highlight the best model w.r.t the testing accuracy. **(10 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_stats(stats, learning_rate, hidden_size, reg):\n",
    "    # Plot the loss function and train / validation accuracies\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history, learning_rate = {}, hidden_size = {}, reg = {}'.format(learning_rate, hidden_size, reg))\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(stats['train_acc_history'], label='train')\n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Clasification accuracy')\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "BvDXDT8N7sc3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T07:02:08.693145Z",
     "start_time": "2022-11-28T06:47:08.623196Z"
    },
    "id": "WU33Q_kwPwdW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "baa11796-f5e7-4be2-d041-cdca3a742561"
   },
   "outputs": [],
   "source": [
    "# This might take some time, try to expirement with small number of testing parameters before continuing\n",
    "# You are encouraged to experiment with additional values\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "regularizations = [0, 0.001, 0.1, 0.25] \n",
    "\n",
    "results = {}\n",
    "best_val = -1   \n",
    "best_net = None \n",
    "################################################################################\n",
    "#                            START OF YOUR CODE                                #\n",
    "################################################################################\n",
    "D = get_num_features(X_train)\n",
    "num_classes = np.unique(y_train).size\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for reg in regularizations:\n",
    "            current_net = ThreeLayerNet(D, hidden_size, num_classes) \n",
    "            #'loss_history', 'train_acc_history', 'val_acc_history'\n",
    "            current_history = current_net.train(X_train, y_train, X_val, y_val, num_iters=1500, batch_size=200, learning_rate = lr, reg = reg, verbose = False)\n",
    "            #plot current model\n",
    "            print('(learning_rate, hidden_size, regularization) = {}'.format((lr, hidden_size, reg)))\n",
    "            plot_stats(current_history, lr, hidden_size, reg)\n",
    "\n",
    "            # val/train accuracy\n",
    "            val_acc = (current_net.predict(X_val) == y_val).mean()\n",
    "            print ('Validation accuracy: ', val_acc)\n",
    "            train_acc = (current_net.predict(X_train) == y_train).mean()\n",
    "            print ('Training accuracy: ', train_acc)\n",
    "            # save in results\n",
    "            results[(lr, hidden_size, reg)] = (train_acc, val_acc)\n",
    "            # check and update best net and val\n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "                best_net = current_net\n",
    "print(\"DONE\")\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "for lr, hidden_size, reg  in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, hidden_size, reg)]\n",
    "    print ('lr %e hidden_size %f reg %f train accuracy: %f val accuracy: %f' % (\n",
    "                lr, hidden_size, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "test_accuracy = (model.predict(X_test) == y_test).mean()\n",
    "print ('Neural Network on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkyH1MtdPwdW"
   },
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg2CJpaA8o1p"
   },
   "source": [
    "##**Question:** \n",
    "What can you say about the training? Why does it take much longer to train (compare to hw1)? **(5 Points)**\n",
    "\n",
    "**Your answer:** we can see that the training works but takes much longer, It takes longer to train a deep network with multiple hidden layers than a simple machine learning algorithm like binary cross-entropy for a few reasons. First, deep networks with multiple hidden layers have more parameters to optimize, which can increase the computational cost of training the network. Second, training deep networks can be more difficult because the gradients calculated during backpropagation can become very small as they are passed through each layer, making it hard for the network to learn. This is known as the vanishing gradient problem. Finally, deep networks with multiple hidden layers can have a more complex decision boundary, which can make it harder for the network to find the optimal set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6TmbEiu8o1p"
   },
   "source": [
    "##**Question:** \n",
    "\n",
    "What can you say about the diffrence (or lack of thereof) between the validation and training accuracy? What can you say about the connection between the loss and the accuracy? **(5 Points)**\n",
    "\n",
    "**Your answer:** The difference between the validation accuracy and the training accuracy can tell you how well the model is performing on unseen data, compared to how well it is performing on the training data. If the validation accuracy is significantly lower than the training accuracy, it can indicate that the model is overfitting to the training data and may not generalize well to new data.\n",
    "\n",
    "The relationship between loss and accuracy is not always straightforward. Loss is a measure of how well the model is able to predict the correct labels for a given input, while accuracy is a measure of how many predictions the model gets right. In general, as the loss decreases, the accuracy of the model will improve, but this is not always the case. For example, a model might have a very low loss but a poor accuracy if it consistently predicts the same label for every input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh8-pdw3-3u7"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Oa-bjEWl9fW6",
    "8MeVWzF19mVA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
